{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ad540b-69c1-43ad-9444-2ee611370b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, GATConv, TransformerConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e7a9f64-f79a-45c5-9c9c-73fda3aac3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Positional Encoding for Graph Nodes: GraphPositionalEncoding\n",
    "class GraphPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Add positional encoding to graph nodes (temporal + spatial)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_nodes=1000, max_time=500):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Temporal positional encoding\n",
    "        self.temporal_pe = nn.Parameter(torch.zeros(max_time, d_model // 2))\n",
    "\n",
    "        # Spatial positional encoding (learnable)\n",
    "        self.spatial_pe = nn.Parameter(torch.zeros(max_nodes, d_model // 2))\n",
    "\n",
    "        self._init_positional_encoding()\n",
    "\n",
    "    def _init_positional_encoding(self):\n",
    "        \"\"\"Initialize with sinusoidal encoding\"\"\"\n",
    "        max_time = self.temporal_pe.shape[0]\n",
    "        d_model = self.temporal_pe.shape[1]\n",
    "\n",
    "        position = torch.arange(0, max_time, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                            (-math.log(10000.0) / d_model))\n",
    "\n",
    "        self.temporal_pe.data[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.temporal_pe.data[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Initialize spatial randomly (will be learned)\n",
    "        nn.init.normal_(self.spatial_pe, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x, node_ids, time_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (num_nodes, feature_dim)\n",
    "            node_ids: (num_nodes,) - spatial node identifiers\n",
    "            time_ids: (num_nodes,) - temporal timestep identifiers\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Get temporal and spatial encodings\n",
    "        temporal_enc = self.temporal_pe[time_ids]  # (num_nodes, d_model//2)\n",
    "        spatial_enc = self.spatial_pe[node_ids]    # (num_nodes, d_model//2)\n",
    "\n",
    "        # Concatenate temporal and spatial\n",
    "        pe = torch.cat([temporal_enc, spatial_enc], dim=-1)  # (num_nodes, d_model)\n",
    "\n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "020b8048-d8bf-4566-9c80-1e900742a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Graph Transformer Encoder Layer: GraphTransformerLayer\n",
    "class GraphTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer layer that operates on graph structure\n",
    "    Combines GNN message passing with transformer attention\n",
    "    (Basically sparser version of multiheaded transformer over the actual spatial network)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead=8, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Graph convolution for local structure\n",
    "        self.graph_conv = GATConv(d_model, d_model, heads=nhead,\n",
    "                                   concat=False, dropout=dropout)\n",
    "\n",
    "        # Self-attention for global dependencies\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead,\n",
    "                                               dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (num_nodes, d_model) - node features\n",
    "            edge_index: (2, num_edges) - graph connectivity\n",
    "            batch: (num_nodes,) - batch assignment\n",
    "            edge_attr: (num_edges, edge_dim) - edge features\n",
    "        \"\"\"\n",
    "        # Graph convolution for local structure\n",
    "        x2 = self.graph_conv(x, edge_index, edge_attr)\n",
    "        x = x + self.dropout1(x2)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Self-attention for global context\n",
    "        # Group nodes by graph for attention\n",
    "        batch_size = batch.max().item() + 1\n",
    "        max_nodes = max([(batch == i).sum() for i in range(batch_size)])\n",
    "\n",
    "        # Create padded batch for attention\n",
    "        x_batched = torch.zeros(batch_size, max_nodes, x.shape[1],\n",
    "                               device=x.device)\n",
    "        mask = torch.ones(batch_size, max_nodes, dtype=torch.bool,\n",
    "                         device=x.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            nodes_in_graph = (batch == i).sum()\n",
    "            x_batched[i, :nodes_in_graph] = x[batch == i]\n",
    "            mask[i, :nodes_in_graph] = False\n",
    "\n",
    "        # Apply self-attention\n",
    "        x_attn, _ = self.self_attn(x_batched, x_batched, x_batched,\n",
    "                                    key_padding_mask=mask)\n",
    "\n",
    "        # Unpack back to node format\n",
    "        x_attn_unpacked = torch.zeros_like(x)\n",
    "        for i in range(batch_size):\n",
    "            nodes_in_graph = (batch == i).sum()\n",
    "            x_attn_unpacked[batch == i] = x_attn[i, :nodes_in_graph]\n",
    "\n",
    "        x = x + self.dropout2(x_attn_unpacked)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Feedforward\n",
    "        x2 = self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "        x = x + self.dropout3(x2)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd1a26ba-678c-43cd-a9c8-55df1a6e1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Spatiotemporal Graph Transformer\n",
    "class SpatiotemporalGraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Main model: Embeds spatiotemporal graphs using transformer architecture\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, node_feature_dim, d_model=256, nhead=8,\n",
    "                 num_layers=6, dim_feedforward=1024, dropout=0.1,\n",
    "                 num_classes=None, max_nodes=1000, max_time=500,\n",
    "                 use_layer_norm=True, use_dropout=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        # Input projection with dropout\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(node_feature_dim, d_model),\n",
    "            nn.LayerNorm(d_model) if use_layer_norm else nn.Identity(),\n",
    "            nn.Dropout(dropout) if use_dropout else nn.Identity()\n",
    "        )\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = GraphPositionalEncoding(d_model, max_nodes, max_time)\n",
    "\n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphTransformerLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Pooling types for graph-level representation\n",
    "        self.pooling_type = 'attention'  # 'mean', 'max', 'attention', 'hierarchical'\n",
    "\n",
    "        if self.pooling_type == 'attention':\n",
    "            self.attention_pool = nn.Sequential(\n",
    "                nn.Linear(d_model, d_model // 2),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model // 2, 1)\n",
    "            )\n",
    "\n",
    "        # Classification head (if num_classes provided) with stronger regularization\n",
    "        if num_classes is not None:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model, d_model // 2),\n",
    "                nn.LayerNorm(d_model // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout * 1.5),  # Stronger dropout\n",
    "                nn.Linear(d_model // 2, d_model // 4),\n",
    "                nn.LayerNorm(d_model // 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout * 1.5),\n",
    "                nn.Linear(d_model // 4, num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, data, return_embeddings=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: PyG Data/Batch object with:\n",
    "                - x: (num_nodes, node_feature_dim)\n",
    "                - edge_index: (2, num_edges)\n",
    "                - batch: (num_nodes,)\n",
    "                - node_ids: (num_nodes,) - spatial identifiers\n",
    "                - time_ids: (num_nodes,) - temporal identifiers\n",
    "                - edge_attr: (num_edges, edge_dim) - optional\n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes) or embeddings: (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None\n",
    "\n",
    "        # Input projection\n",
    "        x = self.input_proj(x)  # (num_nodes, d_model)\n",
    "\n",
    "        # Add positional encoding\n",
    "        if hasattr(data, 'node_ids') and hasattr(data, 'time_ids'):\n",
    "            pos_enc = self.pos_encoder(x, data.node_ids, data.time_ids)\n",
    "            x = x + pos_enc\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, batch, edge_attr)\n",
    "\n",
    "        # Graph-level pooling\n",
    "        graph_embeddings = self._pool_graph(x, batch)\n",
    "\n",
    "        if return_embeddings:\n",
    "            return graph_embeddings\n",
    "\n",
    "        # Classification\n",
    "        if self.num_classes is not None:\n",
    "            logits = self.classifier(graph_embeddings)\n",
    "            return logits\n",
    "\n",
    "        return graph_embeddings\n",
    "\n",
    "    def _pool_graph(self, x, batch):\n",
    "        \"\"\"Pool node features to graph-level representation\"\"\"\n",
    "        if self.pooling_type == 'mean':\n",
    "            return global_mean_pool(x, batch)\n",
    "\n",
    "        elif self.pooling_type == 'max':\n",
    "            return global_max_pool(x, batch)\n",
    "\n",
    "        elif self.pooling_type == 'attention':\n",
    "            # Attention-based pooling\n",
    "            attention_scores = self.attention_pool(x)  # (num_nodes, 1)\n",
    "\n",
    "            batch_size = batch.max().item() + 1\n",
    "            graph_embeddings = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                mask = (batch == i)\n",
    "                node_features = x[mask]  # (nodes_in_graph, d_model)\n",
    "                scores = attention_scores[mask]  # (nodes_in_graph, 1)\n",
    "\n",
    "                # Softmax over nodes in this graph\n",
    "                weights = F.softmax(scores, dim=0)\n",
    "\n",
    "                # Weighted sum\n",
    "                graph_emb = (node_features * weights).sum(dim=0)\n",
    "                graph_embeddings.append(graph_emb)\n",
    "\n",
    "            return torch.stack(graph_embeddings)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling type: {self.pooling_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1b8f33-9974-4eae-9455-a0a74ff6615c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 5. Training Pipeline: train_epoch\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, epoch=None):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch} [Train]' if epoch else 'Training',\n",
    "                leave=True, ncols=100)\n",
    "\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits, batch.y.squeeze())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (important for stability on MPS)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics - move to CPU for accumulation (avoids MPS memory issues)\n",
    "        total_loss += loss.item()\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == batch.y.squeeze()).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "\n",
    "        # Update progress bar\n",
    "        current_loss = total_loss / (batch_idx + 1)\n",
    "        current_acc = correct / total\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{current_loss:.4f}',\n",
    "            'acc': f'{current_acc:.4f}'\n",
    "        })\n",
    "\n",
    "        # Clear cache periodically on MPS to avoid memory issues\n",
    "        if device.type == 'mps' and total % 100 == 0:\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    pbar.close()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "# model evaluator: evaluate\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device, epoch=None, split='Val'):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch} [{split}]' if epoch else f'{split}',\n",
    "                leave=True, ncols=100)\n",
    "\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits, batch.y.squeeze())\n",
    "\n",
    "        # Move metrics to CPU\n",
    "        total_loss += loss.item()\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == batch.y.squeeze()).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "\n",
    "        # Update progress bar\n",
    "        current_loss = total_loss / (batch_idx + 1)\n",
    "        current_acc = correct / total\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{current_loss:.4f}',\n",
    "            'acc': f'{current_acc:.4f}'\n",
    "        })\n",
    "\n",
    "        # Clear cache on MPS\n",
    "        if device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    pbar.close()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ac1e85-8a66-4609-9815-02a0e09051e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Complete Training Pipeline: create_example_data\n",
    "def create_example_data(num_graphs=1000, num_nodes=50, num_timesteps=10,\n",
    "                       node_feature_dim=32, num_classes=5):\n",
    "    \"\"\"\n",
    "    Create example spatiotemporal graph data\n",
    "    Each graph represents a trajectory with temporal snapshots\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_graphs):\n",
    "        # Create temporal graph\n",
    "        total_nodes = num_nodes * num_timesteps\n",
    "\n",
    "        # Node features (num_nodes * num_timesteps, feature_dim)\n",
    "        x = torch.randn(total_nodes, node_feature_dim)\n",
    "        print(x.shape)\n",
    "        \n",
    "        # Create spatiotemporal edges\n",
    "        edge_index = []\n",
    "\n",
    "        # Spatial edges (within each timestep)\n",
    "        for t in range(num_timesteps):\n",
    "            offset = t * num_nodes\n",
    "            for i in range(num_nodes):\n",
    "                # Connect to spatial neighbors\n",
    "                neighbors = [(i + 1) % num_nodes, (i - 1) % num_nodes]\n",
    "                for j in neighbors:\n",
    "                    edge_index.append([offset + i, offset + j])\n",
    "\n",
    "        # Temporal edges (across timesteps)\n",
    "        for t in range(num_timesteps - 1):\n",
    "            for i in range(num_nodes):\n",
    "                curr_idx = t * num_nodes + i\n",
    "                next_idx = (t + 1) * num_nodes + i\n",
    "                edge_index.append([curr_idx, next_idx])\n",
    "                edge_index.append([next_idx, curr_idx])  # bidirectional\n",
    "\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "\n",
    "        # Node and time IDs\n",
    "        node_ids = torch.arange(num_nodes).repeat(num_timesteps)\n",
    "        time_ids = torch.arange(num_timesteps).repeat_interleave(num_nodes)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(x=x, edge_index=edge_index,\n",
    "                   node_ids=node_ids, time_ids=time_ids)\n",
    "\n",
    "        graphs.append(data)\n",
    "        labels.append(torch.randint(0, num_classes, (1,)).item())\n",
    "\n",
    "    return graphs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af90ae30-fa53-45ef-a5af-80e92ecda5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([Data(x=[500, 32], edge_index=[2, 1900], node_ids=[500], time_ids=[500])],\n",
       " [0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_example_data(num_graphs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2326dd5-7c1b-4502-b5fb-1a0fefec2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Features\n",
    "\n",
    "basals = pd.read_pickle('/Users/xies/Library/CloudStorage/OneDrive-Stanford/Skin/Mesa et al/Lineage models/Dataset pickles/basals.pkl')\n",
    "# basals['Region','Meta']\n",
    "\n",
    "basals['Region','Meta'] = [s.split('_')[0] for s in basals.index.get_level_values('TrackID').values]\n",
    "basals['TrackID','Meta'] = [s.split('_')[1] for s in basals.index.get_level_values('TrackID').values]\n",
    "\n",
    "# Split R1 / R2\n",
    "R1 = basals[basals['Region','Meta'] == 'R1']\n",
    "R2 = basals[basals['Region','Meta'] == 'R2']\n",
    "\n",
    "# Load graph\n",
    "adjDicts = [np.load(f'/Users/xies/Library/CloudStorage/OneDrive-Stanford/Skin/Mesa et al/W-R1/Mastodon/basal_connectivity_3d/adjacenct_trackIDs_t{t}.npy'\n",
    "                   , allow_pickle=True).item()\n",
    "            for t in range(15)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06ba1679-f34d-472f-8128-bb7809ab9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each time point to a spatial graph\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "import numpy as np\n",
    "\n",
    "adj_dict = np.load('/Users/allisonlam/Downloads/adjacenct_trackIDs_t0.npy', allow_pickle=True).item()\n",
    "\n",
    "def dict_to_graph(adj_dict, undirected=True, node_features=None):\n",
    "    \"\"\"\n",
    "    Convert a dictionary of adjacency lists into a PyTorch Geometric graph.\n",
    "\n",
    "    Args:\n",
    "        adj_dict (dict): {node_id: [neighbor_ids, ...]} mapping.\n",
    "        undirected (bool): If True, add reverse edges.\n",
    "        node_features (torch.Tensor or None): Optional tensor of shape [num_nodes, num_features].\n",
    "\n",
    "    Returns:\n",
    "        torch_geometric.data.Data: Graph data object.\n",
    "    \"\"\"\n",
    "    # Build edge list\n",
    "    edges = []\n",
    "    for src, neighbors in adj_dict.items():\n",
    "        for dst in neighbors:\n",
    "            edges.append((src, dst))\n",
    "\n",
    "    if not edges:\n",
    "        raise ValueError(\"The adjacency dictionary has no edges.\")\n",
    "\n",
    "    # Convert to tensor [2, num_edges]\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Make graph undirected if requested\n",
    "    if undirected:\n",
    "        edge_index = to_undirected(edge_index)\n",
    "\n",
    "    # Create Data object\n",
    "    data = Data(x = list(adj_dict.keys()), edge_index=edge_index)\n",
    "\n",
    "    # Optionally add node features\n",
    "    if node_features is not None:\n",
    "        data.x = node_features\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90c673-a9af-4e93-9485-22bc34ecac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect each time point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93c011-ac9f-493a-a4c7-0ca9c7d9a839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
